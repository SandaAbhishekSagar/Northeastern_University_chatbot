#!/usr/bin/env python3
"""
Comprehensive script to check Chroma Cloud newtest database collections
- Count total collections
- Verify retrieval completeness
- Analyze collection sizes
- Check for any missing or corrupted collections
"""

import os
import sys
import time
from typing import List, Dict, Any
from datetime import datetime

# Add the project root to the path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    from chroma_cloud_config import get_chroma_cloud_client
    from chromadb import CloudClient
    print("ChromaDB cloud modules imported successfully")
except ImportError as e:
    print(f"Import error: {e}")
    sys.exit(1)

def check_newtest_collections_comprehensive():
    """
    Comprehensive check of newtest database collections
    """
    print("=" * 80)
    print("üîç COMPREHENSIVE NEWTEST DATABASE COLLECTION ANALYSIS")
    print("=" * 80)
    print(f"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    try:
        # Get cloud client for newtest database
        print("üì° Connecting to Chroma Cloud newtest database...")
        client = get_chroma_cloud_client()
        print("‚úÖ Connected successfully")
        print()
        
        # Get all collections
        print("üìã Retrieving all collections...")
        start_time = time.time()
        collections = client.list_collections()
        retrieval_time = time.time() - start_time
        
        print(f"‚úÖ Retrieved {len(collections)} collections in {retrieval_time:.2f} seconds")
        print()
        
        # Basic collection info
        print("üìä BASIC COLLECTION INFORMATION:")
        print("-" * 50)
        print(f"Total Collections Found: {len(collections)}")
        print(f"Retrieval Time: {retrieval_time:.2f} seconds")
        print(f"Average Time per Collection: {retrieval_time/len(collections):.3f} seconds")
        print()
        
        # Detailed analysis
        print("üîç DETAILED COLLECTION ANALYSIS:")
        print("-" * 50)
        
        total_documents = 0
        total_metadata = 0
        batch_collections = []
        document_collections = []
        other_collections = []
        
        collection_details = []
        
        for i, collection in enumerate(collections):
            try:
                # Get collection name and basic info
                collection_name = collection.name
                doc_count = collection.count()
                
                # Get metadata
                try:
                    metadata = collection.metadata or {}
                    metadata_count = len(metadata) if isinstance(metadata, dict) else 0
                except:
                    metadata = {}
                    metadata_count = 0
                
                total_documents += doc_count
                total_metadata += metadata_count
                
                # Categorize collections
                if 'batch_' in collection_name.lower():
                    batch_collections.append(collection_name)
                elif 'document' in collection_name.lower():
                    document_collections.append(collection_name)
                else:
                    other_collections.append(collection_name)
                
                collection_details.append({
                    'name': collection_name,
                    'documents': doc_count,
                    'metadata_count': metadata_count,
                    'metadata': metadata
                })
                
                # Print every 100th collection for progress
                if (i + 1) % 100 == 0:
                    print(f"  Processed {i + 1}/{len(collections)} collections...")
                    
            except Exception as e:
                print(f"‚ùå Error processing collection {i}: {e}")
                continue
        
        print(f"‚úÖ Processed all {len(collections)} collections")
        print()
        
        # Summary statistics
        print("üìà SUMMARY STATISTICS:")
        print("-" * 50)
        print(f"Total Collections: {len(collections):,}")
        print(f"Total Documents: {total_documents:,}")
        print(f"Total Metadata Entries: {total_metadata:,}")
        print(f"Average Documents per Collection: {total_documents/len(collections):.1f}")
        print(f"Average Metadata per Collection: {total_metadata/len(collections):.1f}")
        print()
        
        # Collection type breakdown
        print("üìÇ COLLECTION TYPE BREAKDOWN:")
        print("-" * 50)
        print(f"Batch Collections: {len(batch_collections)}")
        print(f"Document Collections: {len(document_collections)}")
        print(f"Other Collections: {len(other_collections)}")
        print()
        
        # Top 10 largest collections
        print("üèÜ TOP 10 LARGEST COLLECTIONS:")
        print("-" * 50)
        sorted_collections = sorted(collection_details, key=lambda x: x['documents'], reverse=True)
        for i, col in enumerate(sorted_collections[:10]):
            print(f"{i+1:2d}. {col['name']:<40} {col['documents']:>8,} docs")
        print()
        
        # Empty collections check
        empty_collections = [col for col in collection_details if col['documents'] == 0]
        if empty_collections:
            print("‚ö†Ô∏è  EMPTY COLLECTIONS:")
            print("-" * 50)
            print(f"Found {len(empty_collections)} empty collections:")
            for col in empty_collections[:10]:  # Show first 10
                print(f"  - {col['name']}")
            if len(empty_collections) > 10:
                print(f"  ... and {len(empty_collections) - 10} more")
            print()
        
        # Batch collection analysis
        if batch_collections:
            print("üîÑ BATCH COLLECTION ANALYSIS:")
            print("-" * 50)
            batch_docs = sum(col['documents'] for col in collection_details if 'batch_' in col['name'].lower())
            print(f"Total Batch Collections: {len(batch_collections)}")
            print(f"Total Batch Documents: {batch_docs:,}")
            print(f"Average Batch Size: {batch_docs/len(batch_collections):.1f} docs")
            
            # Show batch size distribution
            batch_sizes = [col['documents'] for col in collection_details if 'batch_' in col['name'].lower()]
            if batch_sizes:
                print(f"Largest Batch: {max(batch_sizes):,} docs")
                print(f"Smallest Batch: {min(batch_sizes):,} docs")
            print()
        
        # Retrieval completeness check
        print("‚úÖ RETRIEVAL COMPLETENESS CHECK:")
        print("-" * 50)
        print(f"Collections Retrieved: {len(collections)}")
        print(f"Collections Processed: {len(collection_details)}")
        print(f"Success Rate: {len(collection_details)/len(collections)*100:.1f}%")
        
        if len(collection_details) == len(collections):
            print("‚úÖ All collections retrieved and processed successfully!")
        else:
            print(f"‚ö†Ô∏è  {len(collections) - len(collection_details)} collections failed to process")
        print()
        
        # Performance metrics
        print("‚ö° PERFORMANCE METRICS:")
        print("-" * 50)
        print(f"Total Retrieval Time: {retrieval_time:.2f} seconds")
        print(f"Collections per Second: {len(collections)/retrieval_time:.1f}")
        print(f"Documents per Second: {total_documents/retrieval_time:.0f}")
        print()
        
        # Storage estimate
        print("üíæ STORAGE ESTIMATES:")
        print("-" * 50)
        estimated_storage_mb = total_documents * 0.015  # Rough estimate
        estimated_storage_gb = estimated_storage_mb / 1024
        print(f"Estimated Storage: ~{estimated_storage_mb:.1f} MB ({estimated_storage_gb:.2f} GB)")
        print()
        
        return {
            'total_collections': len(collections),
            'total_documents': total_documents,
            'retrieval_time': retrieval_time,
            'success_rate': len(collection_details)/len(collections)*100,
            'collection_details': collection_details
        }
        
    except Exception as e:
        print(f"‚ùå Error during collection analysis: {e}")
        return None

def test_collection_retrieval_limits():
    """
    Test if there are any limits on collection retrieval
    """
    print("üî¨ TESTING COLLECTION RETRIEVAL LIMITS:")
    print("-" * 50)
    
    try:
        client = get_chroma_cloud_client()
        
        # Test 1: Basic list_collections()
        print("Test 1: Basic list_collections()")
        start_time = time.time()
        collections = client.list_collections()
        basic_time = time.time() - start_time
        print(f"  ‚úÖ Retrieved {len(collections)} collections in {basic_time:.2f}s")
        
        # Test 2: Check if we can access collection metadata
        print("\nTest 2: Collection metadata access")
        sample_collections = collections[:5]  # Test first 5
        for i, collection in enumerate(sample_collections):
            try:
                metadata = collection.metadata
                print(f"  ‚úÖ Collection {i+1}: {collection.name} - metadata accessible")
            except Exception as e:
                print(f"  ‚ùå Collection {i+1}: {collection.name} - metadata error: {e}")
        
        # Test 3: Check collection count access
        print("\nTest 3: Collection count access")
        for i, collection in enumerate(sample_collections):
            try:
                count = collection.count()
                print(f"  ‚úÖ Collection {i+1}: {collection.name} - {count:,} documents")
            except Exception as e:
                print(f"  ‚ùå Collection {i+1}: {collection.name} - count error: {e}")
        
        print(f"\n‚úÖ All retrieval tests completed successfully")
        print(f"üìä Total collections accessible: {len(collections)}")
        
    except Exception as e:
        print(f"‚ùå Error during retrieval limit tests: {e}")

def main():
    """
    Main function to run comprehensive collection analysis
    """
    print("üöÄ Starting comprehensive newtest database collection analysis...")
    print()
    
    # Run comprehensive analysis
    results = check_newtest_collections_comprehensive()
    
    if results:
        print("=" * 80)
        print("üìã FINAL SUMMARY:")
        print("=" * 80)
        print(f"‚úÖ Total Collections: {results['total_collections']:,}")
        print(f"‚úÖ Total Documents: {results['total_documents']:,}")
        print(f"‚úÖ Retrieval Time: {results['retrieval_time']:.2f} seconds")
        print(f"‚úÖ Success Rate: {results['success_rate']:.1f}%")
        print()
        
        # Test retrieval limits
        test_collection_retrieval_limits()
        
        print()
        print("üéâ Analysis completed successfully!")
        
        # Recommendations
        print("\nüí° RECOMMENDATIONS:")
        print("-" * 30)
        if results['success_rate'] == 100:
            print("‚úÖ All collections retrieved successfully - database is healthy")
        else:
            print("‚ö†Ô∏è  Some collections failed to process - check for corrupted data")
        
        if results['total_collections'] > 1000:
            print("üìà Large number of collections - consider batch processing for operations")
        
        print("üîÑ Regular monitoring recommended to track collection growth")
        
    else:
        print("‚ùå Analysis failed - check connection and permissions")

if __name__ == "__main__":
    main()
